input {
  # Polling per i dati di prezzo da Alpha Vantage
  http_poller {
    urls => {
      # Usiamo le variabili d'ambiente per le chiavi API
      alphavantage => "https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&interval=5min&apikey=${ALPHA_VANTAGE_API_KEY}"
    }
    request_timeout => 60
    # Esegui ogni 15 minuti (adatta l'intervallo ai limiti della tua API)
    schedule => { every => "15m" }
    codec => "json"
    # Aggiunge un campo per identificare facilmente questi dati
    add_field => { "data_type" => "stock_price" }
  }

  # Polling per le notizie da NewsAPI
  http_poller {
    urls => {
      newsapi => "https://newsapi.org/v2/everything?q=stocks&language=it&sortBy=publishedAt&apiKey=${NEWSAPI_KEY}"
    }
    request_timeout => 60
    # Esegui ogni 30 minuti
    schedule => { every => "30m" }
    codec => "json"
    add_field => { "data_type" => "news" }
  }

  # Per Reddit, useremo il tuo script Python esistente, che gestisce meglio l'autenticazione.
  # Il plugin 'exec' esegue un comando a intervalli regolari.
  exec {
    command => "python3 /path/in/container/test_reddit.py" # Dovrai montare anche lo script nel container
    interval => 900 # Esegui ogni 15 minuti (900 secondi)
    codec => "json_lines"
    add_field => { "data_type" => "reddit_post" }
  }
}

filter {
  # Per ora, non applichiamo filtri complessi.
  # Potremmo usarlo in futuro per pulire o strutturare i dati.
  mutate {
    # Rimuove i metadati generati da http_poller che non ci servono
    remove_field => ["@version", "host", "@timestamp", "event"]
  }
}

output {
  # Invia tutto a un topic Kafka
  kafka {
    bootstrap_servers => "kafka:29092"
    topic_id => "financial_data_stream"
  }

  # Aggiungi questo output per il debug iniziale: stampa tutto nella console di Logstash
  stdout {
    codec => rubydebug
  }
}